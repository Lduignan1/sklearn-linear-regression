{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrAXg8lWWJV5"
      },
      "source": [
        "## Loading and vectorizing texts with sklearn\n",
        "\n",
        "Scikit-learn has methods to transform a collection of documents into matrices of \"**bag of words**\" representations of these documents.\n",
        "\n",
        "These matrices use the scipy.sparse type, which is appropriate for **sparse matrices**.\n",
        "\n",
        "These modules have 3 methods:\n",
        "- fit : builds the vocabulary and the correspondance between word forms and word ids\n",
        "- transform : transforms the documents into matrices of counts\n",
        "- fit_transform : performs both actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gdDntwTWJWN",
        "outputId": "7b6ca6c3-fd04-407f-a779-a78a3e79d0f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type of X_train <class 'scipy.sparse._csr.csr_matrix'>\n",
            "shape of X_train (4, 15)\n",
            "  (0, 1)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 2)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 13)\t1\n",
            "  (1, 2)\t2\n",
            "  (1, 0)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 9)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 14)\t1\n",
            "  (2, 8)\t1\n",
            "  (2, 12)\t1\n",
            "  (3, 4)\t1\n",
            "  (3, 2)\t1\n",
            "  (3, 8)\t2\n",
            "  (3, 11)\t1\n",
            "  (3, 6)\t1\n",
            "  (3, 10)\t1\n",
            "  (3, 7)\t1\n",
            "[[0 1 1 0 1 0 0 0 0 0 0 0 0 1 0]\n",
            " [1 0 2 1 1 0 0 0 0 1 0 0 0 1 0]\n",
            " [0 0 0 0 0 1 0 0 1 0 0 0 1 0 1]\n",
            " [0 0 1 0 1 0 1 1 2 0 1 1 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# a French corpus (to see what is going on with diacritics)\n",
        "train_corpus = [\n",
        "     'Ceci est un document.',\n",
        "     'Ce document est encore un document à moi.',\n",
        "     'Et voilà le troisième.',\n",
        "     'Le premier document est-il le plus intéressant?',\n",
        " ]\n",
        "vectorizer = CountVectorizer()\n",
        "# the vectorizer is empty : this generates an error\n",
        "#print(vectorizer.vocabulary_)\n",
        "#print(vectorizer.get_feature_names())\n",
        "\n",
        "# we can fill it using the training set\n",
        "# and transform the training set into a matrix\n",
        "X_train = vectorizer.fit_transform(train_corpus)\n",
        "\n",
        "# the matrix is sparse\n",
        "print(\"type of X_train\", type(X_train))\n",
        "print(\"shape of X_train\", X_train.shape)\n",
        "print(X_train)\n",
        "#print(type(X_train))\n",
        "\n",
        "# here it is as a standard matrix\n",
        "print(X_train.toarray()) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1NKBgI5WJWS",
        "outputId": "2c14c663-8aa6-48c9-a683-e58d8ffaf578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ah': 0, 'un': 7, 'nouveau': 6, 'document': 2, 'et': 5, 'ceci': 1, 'est': 4, 'encore': 3}\n",
            "['ah' 'ceci' 'document' 'encore' 'est' 'et' 'nouveau' 'un']\n",
            "\n",
            " Size of vocab: 8\n"
          ]
        }
      ],
      "source": [
        "# here is the mapping between word forms and ids (our \"w2i\" in previous lab session)\n",
        "print(vectorizer.vocabulary_)\n",
        "# the list of word forms (our i2w)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "#QUESTIONS: \n",
        "# What is the size of the vocabulary\n",
        "print(f\"\\n Size of vocab: {len(vectorizer.vocabulary_)}\")\n",
        "# What does the 3rd column of X.train.toarray() represent ? \n",
        "# What is printed when printing the sparse matrix ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The vocabulary is composed of 15 words. Each word has a unique id from 0 to n - 1. These are the values in the `vectorizer.vocabulary_` dictionary. \n",
        "2. The 3rd column represents the number of occurrences of the term `document` in each document of the `train_corpus`. The word `document` occurs two times in the second document therefore the value of the cell in the second row and third column of `X_train` is 2.\n",
        "3. Printing the sparse matrix shows all the non-zero components of the matrix. The tuple represent the cell (row i.e. document, column i.e. word/feature) while the integer is its corresponding value."
      ],
      "metadata": {
        "id": "t-cCgf-rXrzo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HSzAivMpWJWT",
        "outputId": "4489be1e-d8ae-40a8-a4a6-58fea46cbdb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of X_test (2, 8)\n",
            "X_train:\n",
            "[[0 1 1 0 1 0 0 0 0 0 0 0 0 1 0]\n",
            " [1 0 2 1 1 0 0 0 0 1 0 0 0 1 0]\n",
            " [0 0 0 0 0 1 0 0 1 0 0 0 1 0 1]\n",
            " [0 0 1 0 1 0 1 1 2 0 1 1 0 0 0]]\n",
            "\n",
            "X_test:\n",
            "[[1 0 1 0 0 0 1 1]\n",
            " [0 1 1 1 1 1 0 1]]\n",
            "\n",
            "test vocabularyusing fit_transform: {'ah': 0, 'un': 7, 'nouveau': 6, 'document': 2, 'et': 5, 'ceci': 1, 'est': 4, 'encore': 3}\n"
          ]
        }
      ],
      "source": [
        "test_corpus = [ 'Ah un nouveau document.',\n",
        "              'Et ceci est encore un document.']\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_corpus)\n",
        "X_test = vectorizer.fit_transform(test_corpus)\n",
        "print(\"shape of X_test\", X_test.shape)\n",
        "\n",
        "# What happened to the words in test_corpus that are not present in train_corpus?\n",
        "# Compare to vectorizer.fit_transform\n",
        "\n",
        "print(f\"X_train:\\n{X_train.toarray()}\\n\")\n",
        "print(f\"X_test:\\n{X_test.toarray()}\")\n",
        "print(f\"\\ntest vocabularyusing fit_transform: {vectorizer.vocabulary_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The words in `test_corpus` that are not present in `train_corpus` are ignored. This is made apparent when printing both matrices as their row vectors are of equal size (size of vocabulary).\n",
        "2. When calling `fit_transform` on the test corpus this generates new ids for the test set vocab which is clearly smaller than that of the train corpus."
      ],
      "metadata": {
        "id": "EFJiagAFCp1R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VTw6BbKtWJWV",
        "outputId": "359434a7-538f-40e0-c9eb-a818f6ff586d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MEMBERS:\n",
            " ('input', 'content')\n",
            "('encoding', 'utf-8')\n",
            "('decode_error', 'strict')\n",
            "('strip_accents', None)\n",
            "('preprocessor', None)\n",
            "('tokenizer', None)\n",
            "('analyzer', 'word')\n",
            "('lowercase', True)\n",
            "('token_pattern', '(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
            "('stop_words', None)\n",
            "('max_df', 1.0)\n",
            "('min_df', 1)\n",
            "('max_features', None)\n",
            "('ngram_range', (1, 1))\n",
            "('vocabulary', None)\n",
            "('binary', False)\n",
            "('dtype', <class 'numpy.int64'>)\n",
            "('fixed_vocabulary_', False)\n",
            "('_stop_words_id', 9488912)\n",
            "('stop_words_', set())\n",
            "('vocabulary_', {'ah': 0, 'un': 7, 'nouveau': 6, 'document': 2, 'et': 5, 'ceci': 1, 'est': 4, 'encore': 3})\n"
          ]
        }
      ],
      "source": [
        "train_corpus = [\n",
        "     'Ceci est un document .',\n",
        "     'Ce document est encore un document à moi .',\n",
        "     'Et voilà le troisième .',\n",
        "     'Le premier document est -il le plus intéressant ?',\n",
        " ]\n",
        "\n",
        "# QUESTIONS:\n",
        "\n",
        "# How can you change the tokenization that the CountVectorizer will use (see its constructor)?\n",
        "# in particular, how to split on spaces only\n",
        "#  (which corresponds to supposing texts were already tokenized)\n",
        "# Indications: study \n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "# to see all the members of the instance, \n",
        "# and deduce which member to modify:\n",
        "print(\"\\nMEMBERS:\\n\", \"\\n\".join([ str(x) for x in vectorizer.__dict__.items()]))\n",
        "\n",
        "\n",
        "# Which parameters can you modify to switch to bigram and trigram of characters features ?\n",
        "\n",
        "# Search what is a TF.IDF weighting (very famous)\n",
        "\n",
        "# Study the TfidfVectorizer class\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "# dans deduce how to obtain TF.IDF weigthed vector representations of the documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We can change `CountVectorizer`'s tokenization by modifying the value of it's `tokenizer` parameter (default `None`). We can define a new tokenizer which only tokenizes based on whitespace using the `re` module below."
      ],
      "metadata": {
        "id": "HcWi7Lbc0LDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def my_tokenizer(text):\n",
        "  # split based on whitespace\n",
        "  return re.split(\"\\\\s+\",text)\n",
        "\n",
        "my_vectorizer = CountVectorizer(tokenizer=my_tokenizer)"
      ],
      "metadata": {
        "id": "-6YL2u4M-3zx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Two represent features using bigrams or trigrams we can modify the `ngram_range` parameter. An `n_gram_range` of `(2, 2)` considers only bigrams and `(3, 3)` only trigrams while `(1, 3)` considers n-grams with n <= 3."
      ],
      "metadata": {
        "id": "yEMwrVgzAVjn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vEEnlLRiWJWb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a88b8961-701b-46a1-8b23-0667a2aae3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ce' 'ceci' 'document' 'encore' 'est' 'et' 'il' 'intéressant' 'le' 'moi'\n",
            " 'plus' 'premier' 'troisième' 'un' 'voilà']\n",
            "[[0.         0.64065543 0.40892206 0.         0.40892206 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.5051001  0.        ]\n",
            " [0.4203817  0.         0.53664838 0.4203817  0.26832419 0.\n",
            "  0.         0.         0.         0.4203817  0.         0.\n",
            "  0.         0.33143376 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.52547275\n",
            "  0.         0.         0.41428875 0.         0.         0.\n",
            "  0.52547275 0.         0.52547275]\n",
            " [0.         0.         0.23622136 0.         0.23622136 0.\n",
            "  0.37008641 0.37008641 0.58356075 0.         0.37008641 0.37008641\n",
            "  0.         0.         0.        ]]\n",
            "  (0, 2)\t0.4089220628888078\n",
            "  (0, 13)\t0.5051001005334584\n",
            "  (0, 4)\t0.4089220628888078\n",
            "  (0, 1)\t0.6406554311067799\n",
            "  (1, 9)\t0.42038169507735806\n",
            "  (1, 3)\t0.42038169507735806\n",
            "  (1, 0)\t0.42038169507735806\n",
            "  (1, 2)\t0.536648381033003\n",
            "  (1, 13)\t0.33143375695602084\n",
            "  (1, 4)\t0.2683241905165015\n",
            "  (2, 12)\t0.5254727492640658\n",
            "  (2, 8)\t0.41428875116588965\n",
            "  (2, 14)\t0.5254727492640658\n",
            "  (2, 5)\t0.5254727492640658\n",
            "  (3, 7)\t0.3700864064422486\n",
            "  (3, 10)\t0.3700864064422486\n",
            "  (3, 6)\t0.3700864064422486\n",
            "  (3, 11)\t0.3700864064422486\n",
            "  (3, 8)\t0.5835607473961767\n",
            "  (3, 2)\t0.2362213592851699\n",
            "  (3, 4)\t0.2362213592851699\n"
          ]
        }
      ],
      "source": [
        "# getting tf-idf weighted vector representations of the documents\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(train_corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "# printing new matrix tf-idf vectors\n",
        "print(X.toarray())\n",
        "\n",
        "# printing the sparse matrix\n",
        "print(X)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}